%%%
 % File: /latex/big-cocluster-paper/introduction.tex
 % Created Date: Tuesday, December 26th, 2023
 % Author: Zihan
 % -----
 % Last Modified: Wednesday, 27th March 2024 10:36:14 pm
 % Modified By: the developer formerly known as Zihan at <wzh4464@gmail.com>
 % -----
 % HISTORY:
 % Date      		By   	Comments
 % ----------		------	---------------------------------------------------------
%%%

\section{Introduction}
Artificial Intelligence (AI) is an intense focus technology that reveals the most rapidly growing process on key technology and has a tremendous impact on an extensive range of domains from the health sector, as it supports complex data analysis, recognition of the patterns, and the processes leading to decision-making. Clustering is the execution of one of the significant roles within AI and involves a number of unsupervised learning techniques in the support of data categorization and pointing out patterns. Clustering algorithms, such as $k$-means\cite{lloyd1982LeastSquaresQuantization, macqueen1967MethodsClassificationAnalysis} and Gaussian Mixture Model (GMM)\cite{dempster1977MaximumLikelihoodIncomplete}, serve to aggregate data points based on shared attributes, thereby simplifying and interpreting the underlying data structure. However, these clustering algorithms often grapple with limitations, particularly when handling large, high-dimensional datasets. Their tendency to consider all features of data objects uniformly often leads to oversimplified interpretations and overlooks more nuanced, context-specific relationships within the data\cite{chen2023FastFlexibleBipartite, zhao2023MultiviewCoclusteringMultisimilarity, kumar2023CoclusteringBasedMethods}.

\textit{Co-clustering}\cite{cheng2000BiclusteringExpressionData, kluger2003SpectralBiclusteringMicroarray, yan2017CoclusteringMultidimensionalBig} is an approach that addresses these shortcomings by simultaneously grouping both rows (objects) and columns (features), revealing intricate correlations between two distinct data types, unlike other traditional cluster methods\cite{zhang2023AdaptiveGraphConvolution, yuan2023JointNetworkTopology, wu2023EffectiveClusteringStructured} that cluster either rows (objects) or columns (features) in isolation. This approach is particularly transformative in scenarios where the relationships between rows and columns are as crucial as the individual entities themselves. For example, in bioinformatics, co-clustering helps in identifying gene patterns by analyzing genes and conditions concurrently, thereby revealing more comprehensive biological insights\cite{higham2007SpectralClusteringIts, kluger2003SpectralBiclusteringMicroarray, madeira2004BiclusteringAlgorithmsBiological, zhao2012BiclusteringAnalysisPattern, golchev2015BiclusteringAnalysisGene}. Similarly, in text mining and recommender systems, it enables the discovery of latent relationships between users and items or documents and terms\cite{busygin2008BiclusteringDataMining, dhillon2001CoclusteringDocumentsWords, dhillon2007WeightedGraphCuts, chen2023ParallelNonNegativeMatrix, bouchareb2019ModelBasedCoclustering}. By addressing the shortcomings of traditional clustering methods, co-clustering not only enhances accuracy in pattern detection but also broadens the analytical scope, allowing for more sophisticated and context-aware data analysis.

Despite the advancements offered by co-clustering algorithms, they still have several limitations. 1) High Computational Complexity: The computational complexity can be prohibitively high, leading to prolonged processing times. This arises from the need to simultaneously analyze relationships across multiple dimensions, which grows exponentially with data size.
2) Significant Communication Overhead: Even if methods like partitioning are employed to handle large-scale data, the communication overhead remains substantial due to multiple rounds of data exchange. This is because co-clustering algorithms typically involve iterative optimization, requiring communication in each iteration, thus increasing the overall communication overhead.
3) Requirement for Sparse Matrices: Co-clustering algorithms typically require sparse matrices as input, and existing approaches struggle to adapt to dense matrices. This is because the algorithms often rely on the presence of missing values or zero values in the data matrix. Dense matrices may lead to difficulties in effectively handling a large number of non-zero elements.

To address the inherent challenges of traditional co-clustering algorithms, our research introduces an innovative hierarchical, agglomerative co-clustering approach. First,  we propose an optimal big matrix partitioning algorithm. This algorithm uses proposed probabilistic model to determine the optimal number and order of sub-matrices to be partitioned. This strategic partitioning allows for parallel processing of co-clustering tasks, significantly reducing the processing time and lessening the computational and storage demands on each processing unit.  
Second, we propose an efficient hierarchical ensemble method. This method combines results from sub-matrix co-clusterings, leveraging ensemble learning techniques to enhance the accuracy and reliability of the clustering outcomes. Our hierarchical ensemble approach ensures a more robust and consistent clustering performance, addressing the issue of a heterogeneity and model uncertainty. 
These two methodologies-partitioning and ensemble-synergistically work to enable a more scalable, efficient, and precise co-clustering process, particularly in handling large-scale, complex datasets.

The contributions of this paper are summarized as follows:
\begin{enumerate}
    \item \textbf{Hierarchical, Agglomerative Co-clustering Approach: }We present a novel hierarchical, agglomerative co-clustering approach to address the inherent challenges of traditional co-clustering algorithms. We propose an optimal big matrix partitioning algorithm that strategically determines the optimal number and order of sub-matrices to be partitioned, allowing for parallel processing of co-clustering tasks. This significantly reduces processing time and lessens computational and storage demands on each processing unit.
    \item \textbf{Efficient Hierarchical Ensemble Method} We propose an efficient hierarchical ensemble method that combines results from sub-matrix co-clusterings to enhance the accuracy and reliability of clustering outcomes. This approach ensures a more robust and consistent clustering performance, particularly addressing the issue of variability in co-clustering results.
    \item \textbf{Scalable and Precise Co-clustering with Verified Efficiency: } Our method has been validated critically through a wide spectrum of situations that have been undergone to be highly robust and a useful tool in co-clustering large, complex datasets. Our experimental results show a complete paradigm shift in the clustering effectiveness compared to techniques based on conventional methods. 
    % Not surprisingly, in such a case, the remarkably reduced running time in our approach is 1/6 for a dense matrix and in the case of a sparse matrix, the reduction is up to the level of 70\%. 
    We reduced the running time significantly to 1/6 in the scenario of a dense matrix and up to 70\% in the case of a sparse matrix.
    This is an undeniably handsome improvement by the method, which argues for the very high competence of our technique in both types of matrices, but it really makes a point of importance regarding benchmarks for redefinition clustering technology in the world of data analysis. 
\end{enumerate}

The structure of this paper is as follows: Section \ref{sec:related_work} reviews related works; Section \ref{sec:method} describes our novel co-clustering approach; Section \ref{sec:experiment} presents experiments and results, validating the efficacy of our approach; and Section \ref{sec:conclude} concludes the paper, summarizing our findings and discussing potential avenues for future research in this domain.
