% !TeX root = main.tex

%%%
% File: /latex/big-cocluster-paper/related_work.tex
% Created Date: Monday, December 18th, 2023
% Author: Zihan
% -----
% Last Modified: Monday, 18th December 2023 5:23:36 pm
% Modified By: the developer formerly known as Zihan at <wzh4464@gmail.com>
% -----
% HISTORY:
% Date      		By   	Comments
% ----------		------	---------------------------------------------------------
% 19-12-2023		Zihan	Complete the first draft of Related Work
%%%

\section{Related work}
\subsection{Neural Network models}
Dong \textit{et al.}\cite{dongkuanxu2019DeepCoClustering} proposed DeepCC, the first practical deep-learning model for co-clustering. DeepCC employs deep autoencoders and Gaussian Mixture Models to simultaneously cluster instances and features in an end-to-end fashion. A mutual information loss connects the joint instance and feature representation learning.
However, DeepCC faces challenges in computational efficiency and parameter tuning. 
%  {\color{blue} However, DeepCC is limited to static data, because it does not support incremental updates. Besides, DeepCC does not scale well to large datasets due to its high computational complexity.
% Besides, DeepCC does not scale well to large datasets due to its high computational complexity.
% If others said these cons, you need to add ref to show not you diss DeepCC}.

\subsection{Matrix factorization}
%TODO: Overview: \cite{lin2019OverviewCoClusteringMatrix}
Kluger et al. \cite{kluger2003SpectralBiclusteringMicroarray} developed a spectral biclustering method that utilizes Singular Value Decomposition (SVD) to find checkerboard structures corresponding to genes differentially expressed across subsets of conditions in gene expression data.   

A pivotal shift in co-clustering methodology emerged with the adoption of matrix factorization techniques. This approach, fundamentally different from traditional clustering methods, factors the data matrix into multiple matrices, revealing underlying patterns and associations between rows and columns. The introduction of Non-negative Matrix Factorization (NMF) in co-clustering marked a significant advancement. By decomposing the sample-feature matrix into separate matrices for samples and features, NMF-based co-clustering techniques, such as the orthogonal NMTF by Ding \textit{et al.} \cite{ding2006OrthogonalNonnegativeMatrix}, provided a more interpretable and efficient way to identify clusters. 

Following this, various enhancements and extensions to matrix factorization in co-clustering were proposed. Methods like Fast Non-negative Matrix Tri-factorization (FNMTF) \cite{wang2019DualHypergraphRegularized}and Bilateral k-means (BKM) \cite{junweihan2017BilateralKMeansAlgorithm} further refined the approach by introducing constraints and optimizations that improved computational speed and accuracy.  

The matrix factorization-based co-clustering has not only been limited to static data analysis but also extended to dynamic scenarios. This adaptation is evident in applications like real-time collaborative filtering and online text mining, where the ability to update co-clusters incrementally becomes crucial \cite{daruru2009PervasiveParallelismData}.



% Now, to process big data, MTF is the most popular method. \cite{chen2023ParallelNonNegativeMatrix}

 {\color{blue} While matrix factorization techniques have shown promise for co-clustering large datasets, scaling to massive high-dimensional data remains an open challenge. } Chen et al. \cite{chen2023ParallelNonNegativeMatrix} proposed a parallel non-negative matrix tri-factorization method that distributes computation across multiple nodes to accelerate factorizations.  {\color{blue} However, such approaches still struggle with web-scale data.}

Our proposed method takes a divide-and-conquer approach, directly partitioning the input matrix into smaller submatrices before co-clustering each one in parallel. This blocks the original high dimensionality to make co-clustering feasible. The separate results are then ensembled to produce final co-clusters. This represents a new paradigm tailored for big data that sidesteps computational barriers by transforming the problem space rather than relying solely on distributed computing optimizations.