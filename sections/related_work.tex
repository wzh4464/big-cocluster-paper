% !TeX root = main.tex

%%%
% File: /latex/big-cocluster-paper/related_work.tex
% Created Date: Monday, December 18th, 2023
% Author: Zihan
% -----
% Last Modified: Monday, 18th December 2023 5:23:36 pm
% Modified By: the developer formerly known as Zihan at <wzh4464@gmail.com>
% -----
% HISTORY:
% Date      		By   	Comments
% ----------		------	---------------------------------------------------------
% 19-12-2023		Zihan	Complete the first draft of Related Work
%%%

\section{Related work}
\label{sec:related_work}
\subsection{Co-clustering Models}

The conception of \emph{biclustering}, where the row and column of data are clustered simultaneously, had first been introduced in\cite{hartigan1972DirectClusteringData} while analyzing gene expression data. Since then, this conception was further expanded for many other areas as \emph{co-clustering}; several models have been developed to overcome different challenges.

Spectral Co-clustering method with the Singular Value Decomposition (SVD) introduced by  Dhillon \textit{et al.} \cite{dhillon2001CoclusteringDocumentsWords} was then popularized because it is associated with simplicity and effectiveness. This method has given birth to two great directions in co-clustering: graph-based and matrix factorization-based approaches. 

The most widely used graph-based co-clustering method to date is FBGPC\cite{chen2023FastFlexibleBipartitea}: a flexible bipartite graph model applied on the original data. In contrast, the most popular matrix factorization-based method, NMTF\cite{long2005CoclusteringBlockValue}, decomposes the data to simpler matrices to find the underlying patterns. 

DeepCC\cite{dongkuanxu2019DeepCoClustering} is the first deep learning model for co-clustering data, using deep autoencoders and Gaussian Mixture Models for better clustering of data. However, the innovations it brings bear upon the computational efficiency that DeepCC grapples with, besides many kinds of data types. 

However, these methods have limitations with large datasets due to their complex computational demands. Thus parallelizing co-clustering is essential to address these challenges.

\subsection{Parallelizing Co-clustering}

To address the big data challenge, parallelizing co-clustering methods emerged as a solution. 
\cite{cheng2015CoClusterDDistributedFramework} introduced a distributed co-clustering framework, CoClusterD, which alternates minimization co-clustering (AMCC) algorithm with sequential updates and built up a distributed co-clustering framework. However, iterations are not guaranteed in advance thus in-efficiency in computation.

While matrix factorization techniques have shown promise for co-clustering large datasets, scaling to massive high-dimensional data remains an open challenge. Chen \textit{et al.}\cite{chen2023ParallelNonNegativeMatrix} proposed a parallel non-negative matrix tri-factorization method that distributes computation across multiple nodes to accelerate factorizations. However, such approaches still struggle with web-scale data.

Our proposed method takes a divide-and-conquer approach, directly partitioning the input matrix into smaller submatrices before co-clustering each one in parallel. This blocks the original high dimensionality to make co-clustering feasible. The separate results are then ensembled to produce final co-clusters. This represents a new paradigm tailored for big data that sidesteps computational barriers by transforming the problem space rather than relying solely on distributed computing optimizations.